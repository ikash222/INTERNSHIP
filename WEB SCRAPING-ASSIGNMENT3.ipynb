{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e38e7b8",
   "metadata": {},
   "source": [
    "# 1. Write a python program which searches all the product under a particular product from www.amazon.in. The product to be searched will be taken as input from user. For e.g. If user input is ‘guitar’. Then search for guitars. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc72d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def search_amazon(product):\n",
    "    base_url = \"https://www.amazon.in/s?k=\"\n",
    "    search_url = base_url + product.replace(\" \", \"+\")\n",
    "\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "\n",
    "    response = requests.get(search_url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        products = soup.find_all('div', {'data-component-type': 's-search-result'})\n",
    "\n",
    "        if products:\n",
    "            for product in products:\n",
    "                title = product.find('span', {'class': 'a-text-normal'}).text.strip()\n",
    "                price = product.find('span', {'class': 'a-price-whole'})\n",
    "                if price:\n",
    "                    price = price.text.strip()\n",
    "                else:\n",
    "                    price = \"Price not available\"\n",
    "                print(\"Product:\", title)\n",
    "                print(\"Price:\", price)\n",
    "                print(\"=\"*50)\n",
    "        else:\n",
    "            print(\"No products found.\")\n",
    "    else:\n",
    "        print(\"Failed to retrieve Amazon page.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    user_input = input(\"Enter the product to search on Amazon.in: \")\n",
    "    search_amazon(user_input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db73dec4",
   "metadata": {},
   "source": [
    "# 2. In the above question, now scrape the following details of each product listed in first 3 pages of your search results and save it in a data frame and csv. In case if any product has less than 3 pages in search results thenscrape all the products available under that product name. Details to be scraped are:  randName\", \"Name of the Product\", \"Price\", \"Return/Exchange\", \"Expected Delivery\", \"Availability\" and“Product URL”. In case, if any of the details are missing for any of the product then replace it by “-“."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8c90da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def get_product_details(product_url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "    response = requests.get(product_url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        brand_name = soup.find('a', {'id': 'bylineInfo'}).text.strip() if soup.find('a', {'id': 'bylineInfo'}) else '-'\n",
    "        product_name = soup.find('span', {'id': 'productTitle'}).text.strip() if soup.find('span', {'id': 'productTitle'}) else '-'\n",
    "        price = soup.find('span', {'id': 'priceblock_ourprice'}).text.strip() if soup.find('span', {'id': 'priceblock_ourprice'}) else '-'\n",
    "        return_exchange = soup.find('div', {'id': 'RETURNS_POLICY'}).text.strip() if soup.find('div', {'id': 'RETURNS_POLICY'}) else '-'\n",
    "        expected_delivery = soup.find('div', {'id': 'ddmDeliveryMessage'}).text.strip() if soup.find('div', {'id': 'ddmDeliveryMessage'}) else '-'\n",
    "        availability = soup.find('div', {'id': 'availability'}).text.strip() if soup.find('div', {'id': 'availability'}) else '-'\n",
    "        return brand_name, product_name, price, return_exchange, expected_delivery, availability, product_url\n",
    "    else:\n",
    "        return '-', '-', '-', '-', '-', '-', '-'\n",
    "\n",
    "def search_amazon(product):\n",
    "    base_url = \"https://www.amazon.in/s?k=\"\n",
    "    search_url = base_url + product.replace(\" \", \"+\")\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "    \n",
    "    all_products = []\n",
    "    for page_number in range(1, 4):\n",
    "        page_url = search_url + \"&page=\" + str(page_number)\n",
    "        response = requests.get(page_url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            products = soup.find_all('div', {'data-component-type': 's-search-result'})\n",
    "            if not products:\n",
    "                break\n",
    "            for product in products:\n",
    "                product_link = 'https://www.amazon.in' + product.find('a', {'class': 'a-link-normal'})['href']\n",
    "                details = get_product_details(product_link)\n",
    "                all_products.append(details)\n",
    "\n",
    "    return all_products\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    user_input = input(\"Enter the product to search on Amazon.in: \")\n",
    "    products_data = search_amazon(user_input)\n",
    "\n",
    "    \n",
    "    df = pd.DataFrame(products_data, columns=[\"Brand Name\", \"Name of the Product\", \"Price\", \"Return/Exchange\", \"Expected Delivery\", \"Availability\", \"Product URL\"])\n",
    "\n",
    "    \n",
    "    df.to_csv(\"amazon_products.csv\", index=False)\n",
    "\n",
    "    print(\"Scraping and saving complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f93d081",
   "metadata": {},
   "source": [
    "# 3. Write a python program to access the search bar and search button on images.google.com and scrape 10images each for keywords ‘fruits’, ‘cars’ and ‘Machine Learning’, ‘Guitar’, ‘Cakes’. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a94273f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_images(keyword, num_images):\n",
    "    driver = webdriver.Chrome()  \n",
    "    driver.get(\"https://www.google.com/imghp?hl=en&ogbl\")\n",
    "    time.sleep(2)  \n",
    "\n",
    "    \n",
    "    search_bar = driver.find_element_by_xpath(\"//input[@title='Search']\")\n",
    "    search_bar.send_keys(keyword)\n",
    "    search_bar.submit()\n",
    "    time.sleep(2) \n",
    "\n",
    "    \n",
    "    for _ in range(5):\n",
    "        driver.execute_script(\"window.scrollBy(0,1000)\")\n",
    "\n",
    "    \n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    image_elements = soup.find_all('img', {'class': 'rg_i'})\n",
    "\n",
    "    \n",
    "    directory = keyword.replace(\" \", \"_\")\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "    \n",
    "    count = 0\n",
    "    for img in image_elements[:num_images]:\n",
    "        try:\n",
    "            image_url = img['src']\n",
    "            image_data = requests.get(image_url).content\n",
    "            with open(f\"{directory}/image{count + 1}.jpg\", \"wb\") as f:\n",
    "                f.write(image_data)\n",
    "            print(f\"Downloaded image {count + 1} for '{keyword}'\")\n",
    "            count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading image {count + 1} for '{keyword}': {e}\")\n",
    "\n",
    "        if count >= num_images:\n",
    "            break\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    keywords = ['fruits', 'cars', 'Machine Learning', 'Guitar', 'Cakes']\n",
    "    num_images = 10\n",
    "    for keyword in keywords:\n",
    "        scrape_images(keyword, num_images)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b05d95",
   "metadata": {},
   "source": [
    "# 4. Write a python program to search for a smartphone(e.g.: Oneplus Nord, pixel 4A, etc.) on www.flipkart.com and scrape following details for all the   search results displayed on 1st page. Details to be scraped: “Brand Name”, “Smartphone name”, “Colour”, “RAM”, “Storage(ROM)”, “Primary Camera”, “Secondary Camera”, “Display Size”, “Battery Capacity”, “Price”, “Product URL”. Incase if any of the details is missing then replace it by “- “. Save your results in a dataframe and CSV. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a401740f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_flipkart_smartphones(search_query):\n",
    "    base_url = f\"https://www.flipkart.com/search?q={search_query}&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off&page=1\"\n",
    "\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(base_url, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        products = soup.find_all(\"div\", {\"class\": \"_1AtVbE\"})  \n",
    "\n",
    "        results = []\n",
    "\n",
    "        for product in products:\n",
    "            try:\n",
    "                brand_name = product.find(\"div\", {\"class\": \"_4rR01T\"}).text.strip()\n",
    "            except AttributeError:\n",
    "                brand_name = \"-\"\n",
    "\n",
    "            try:\n",
    "                name = product.find(\"a\", {\"class\": \"IRpwTa\"}).text.strip()\n",
    "            except AttributeError:\n",
    "                name = \"-\"\n",
    "\n",
    "            try:\n",
    "                color = product.find(\"a\", {\"class\": \"IRpwTa\"}).text.strip().split(\",\")[1].strip()\n",
    "            except (AttributeError, IndexError):\n",
    "                color = \"-\"\n",
    "\n",
    "            specs = product.find_all(\"li\", {\"class\": \"rgWa7D\"})\n",
    "            specs_dict = {}\n",
    "            for spec in specs:\n",
    "                try:\n",
    "                    key, value = spec.text.split(\":\")\n",
    "                    specs_dict[key.strip()] = value.strip()\n",
    "                except ValueError:\n",
    "                    pass\n",
    "\n",
    "            ram = specs_dict.get(\"RAM\", \"-\")\n",
    "            rom = specs_dict.get(\"ROM\", \"-\")\n",
    "            primary_camera = specs_dict.get(\"Primary Camera\", \"-\")\n",
    "            secondary_camera = specs_dict.get(\"Secondary Camera\", \"-\")\n",
    "            display_size = specs_dict.get(\"Display Size\", \"-\")\n",
    "            battery_capacity = specs_dict.get(\"Battery Capacity\", \"-\")\n",
    "\n",
    "            try:\n",
    "                price = product.find(\"div\", {\"class\": \"_30jeq3 _1_WHN1\"}).text.strip().replace(\"₹\", \"\").replace(\",\", \"\").strip()\n",
    "            except AttributeError:\n",
    "                price = \"-\"\n",
    "\n",
    "            try:\n",
    "                product_url = \"https://www.flipkart.com\" + product.find(\"a\", {\"class\": \"IRpwTa\"})[\"href\"]\n",
    "            except AttributeError:\n",
    "                product_url = \"-\"\n",
    "\n",
    "            result = {\n",
    "                \"Brand Name\": brand_name,\n",
    "                \"Smartphone Name\": name,\n",
    "                \"Colour\": color,\n",
    "                \"RAM\": ram,\n",
    "                \"Storage(ROM)\": rom,\n",
    "                \"Primary Camera\": primary_camera,\n",
    "                \"Secondary Camera\": secondary_camera,\n",
    "                \"Display Size\": display_size,\n",
    "                \"Battery Capacity\": battery_capacity,\n",
    "                \"Price\": price,\n",
    "                \"Product URL\": product_url,\n",
    "            }\n",
    "\n",
    "            results.append(result)\n",
    "\n",
    "        return results\n",
    "    else:\n",
    "        print(\"Failed to fetch data from Flipkart.\")\n",
    "        return []\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    search_query = input(\"Enter the smartphone you want to search for on Flipkart: \")\n",
    "    search_results = scrape_flipkart_smartphones(search_query)\n",
    "\n",
    "    if search_results:\n",
    "        df = pd.DataFrame(search_results)\n",
    "        df.to_csv(f\"{search_query}_flipkart_search_results.csv\", index=False)\n",
    "        print(\"Search results saved successfully.\")\n",
    "    else:\n",
    "        print(\"No search results found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9452133e",
   "metadata": {},
   "source": [
    "# 5. Write a program to scrap geospatial coordinates (latitude, longitude) of a city searched on google maps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046b2aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_coordinates(city):\n",
    "    base_url = \"https://www.google.com/maps/search/\"\n",
    "    search_url = base_url + city.replace(\" \", \"+\")\n",
    "\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "    }\n",
    "\n",
    "    response = requests.get(search_url, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        try:\n",
    "        \n",
    "            url = soup.find(\"meta\", property=\"og:url\")[\"content\"]\n",
    "            latitude, longitude = url.split(\"/@\")[1].split(\",\")[0:2]\n",
    "            return latitude, longitude\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting coordinates: {e}\")\n",
    "            return None, None\n",
    "    else:\n",
    "        print(\"Failed to retrieve data from Google Maps.\")\n",
    "        return None, None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    city = input(\"Enter the city name to search for on Google Maps: \")\n",
    "    latitude, longitude = scrape_coordinates(city)\n",
    "    if latitude and longitude:\n",
    "        print(f\"Coordinates of {city}: Latitude - {latitude}, Longitude - {longitude}\")\n",
    "    else:\n",
    "        print(\"Failed to retrieve coordinates.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a80147",
   "metadata": {},
   "source": [
    "# 6. Write a program to scrap all the available details of best gaming laptops from digit.in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e42fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_gaming_laptops():\n",
    "    url = \"https://www.digit.in/top-products/best-gaming-laptops-40.html\"\n",
    "\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        laptops = soup.find_all('div', class_='TopNumbeHeading sticky-footer')\n",
    "\n",
    "        results = []\n",
    "\n",
    "        for laptop in laptops:\n",
    "            name = laptop.find('div', class_='TopNumbeHeading sticky-footer').text.strip()\n",
    "            specs = laptop.find('div', class_='smprice')\n",
    "\n",
    "            processor = specs.find('div', class_='value').text.strip()\n",
    "            memory = specs.find_all('div', class_='value')[1].text.strip()\n",
    "            os = specs.find_all('div', class_='value')[2].text.strip()\n",
    "            storage = specs.find_all('div', class_='value')[3].text.strip()\n",
    "            display_size = specs.find_all('div', class_='value')[4].text.strip()\n",
    "            price = specs.find('td', class_='smprice').text.strip()\n",
    "            product_url = laptop.find('a')['href']\n",
    "\n",
    "            result = {\n",
    "                \"Name\": name,\n",
    "                \"Processor\": processor,\n",
    "                \"Memory\": memory,\n",
    "                \"Operating System\": os,\n",
    "                \"Storage\": storage,\n",
    "                \"Display Size\": display_size,\n",
    "                \"Price\": price,\n",
    "                \"Product URL\": product_url\n",
    "            }\n",
    "\n",
    "            results.append(result)\n",
    "\n",
    "        return results\n",
    "    else:\n",
    "        print(\"Failed to fetch data from digit.in.\")\n",
    "        return []\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    gaming_laptops_data = scrape_gaming_laptops()\n",
    "\n",
    "    if gaming_laptops_data:\n",
    "        df = pd.DataFrame(gaming_laptops_data)\n",
    "        df.to_csv(\"gaming_laptops_digit.csv\", index=False)\n",
    "        print(\"Scraping complete. Data saved to 'gaming_laptops_digit.csv'\")\n",
    "    else:\n",
    "        print(\"No data found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13b1e06",
   "metadata": {},
   "source": [
    "# 7. Write a python program to scrape the details for all billionaires from www.forbes.com. Details to be scrapped: “Rank”, “Name”, “Net worth”, “Age”, “Citizenship”, “Source”, “Industry”. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff0faa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_billionaires():\n",
    "    url = \"https://www.forbes.com/billionaires/\"\n",
    "\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        billionaires = soup.find_all('div', class_='personName')\n",
    "\n",
    "        results = []\n",
    "\n",
    "        for i, billionaire in enumerate(billionaires, start=1):\n",
    "            name = billionaire.text.strip()\n",
    "            rank = i\n",
    "            net_worth = billionaire.find_next('div', class_='netWorth').text.strip()\n",
    "            age = billionaire.find_next('div', class_='age').text.strip()\n",
    "            citizenship = billionaire.find_next('div', class_='countryOfCitizenship').text.strip()\n",
    "            source = billionaire.find_next('div', class_='source-column').text.strip()\n",
    "            industry = billionaire.find_next('div', class_='category').text.strip()\n",
    "\n",
    "            result = {\n",
    "                \"Rank\": rank,\n",
    "                \"Name\": name,\n",
    "                \"Net Worth\": net_worth,\n",
    "                \"Age\": age,\n",
    "                \"Citizenship\": citizenship,\n",
    "                \"Source\": source,\n",
    "                \"Industry\": industry\n",
    "            }\n",
    "\n",
    "            results.append(result)\n",
    "\n",
    "        return results\n",
    "    else:\n",
    "        print(\"Failed to fetch data from Forbes.com.\")\n",
    "        return []\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    billionaires_data = scrape_billionaires()\n",
    "\n",
    "    if billionaires_data:\n",
    "        df = pd.DataFrame(billionaires_data)\n",
    "        df.to_csv(\"billionaires_forbes.csv\", index=False)\n",
    "        print(\"Scraping complete. Data saved to 'billionaires_forbes.csv'\")\n",
    "    else:\n",
    "        print(\"No data found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d860772",
   "metadata": {},
   "source": [
    "# 8. Write a program to extract at least 500 Comments, Comment upvote and time when comment was posted from any YouTube Video. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcabea45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def fetch_video_comments(video_id, api_key, max_results=500):\n",
    "    base_url = \"https://www.googleapis.com/youtube/v3/commentThreads\"\n",
    "    params = {\n",
    "        \"part\": \"snippet\",\n",
    "        \"videoId\": video_id,\n",
    "        \"key\": api_key,\n",
    "        \"maxResults\": max_results\n",
    "    }\n",
    "\n",
    "    response = requests.get(base_url, params=params)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        comments_data = []\n",
    "        for item in data['items']:\n",
    "            comment = item['snippet']['topLevelComment']['snippet']['textDisplay']\n",
    "            comment_upvotes = item['snippet']['topLevelComment']['snippet']['likeCount']\n",
    "            comment_time = item['snippet']['topLevelComment']['snippet']['publishedAt']\n",
    "            comments_data.append({\n",
    "                'comment': comment,\n",
    "                'upvotes': comment_upvotes,\n",
    "                'time': comment_time\n",
    "            })\n",
    "        return comments_data\n",
    "    else:\n",
    "        print(\"Failed to fetch comments:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    video_id = input(\"Enter the YouTube video ID: \")\n",
    "    api_key = input(\"Enter your YouTube Data API key: \")\n",
    "    comments_data = fetch_video_comments(video_id, api_key)\n",
    "\n",
    "    if comments_data:\n",
    "        print(\"Comments extracted successfully.\")\n",
    "        for idx, comment_info in enumerate(comments_data, start=1):\n",
    "            print(f\"\\nComment {idx}:\")\n",
    "            print(\"Text:\", comment_info['comment'])\n",
    "    \n",
    "    print(\"Upvotes:\", comment_info['upvotes'])\n",
    "            print(\"Time:\", comment_info['time'])\n",
    "    else:\n",
    "        print(\"No comments found or failed to fetch comments.\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711fc19a",
   "metadata": {},
   "source": [
    "# 9. Write a python program to scrape a data for all available Hostels from https://www.hostelworld.com/ in “London” location. You have to scrape hostel name, distance from city centre, ratings, total reviews, overall reviews, privates from price, dorms from price, facilities and property description. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee054f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_hostels(location):\n",
    "    url = f\"https://www.hostelworld.com/findabed.php/ChosenCity.{location}/ChosenCountry.England\"\n",
    "\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        hostels = soup.find_all('div', class_='fabresult')\n",
    "\n",
    "        results = []\n",
    "\n",
    "        for hostel in hostels:\n",
    "            name = hostel.find('h2', class_='title').text.strip()\n",
    "            distance = hostel.find('span', class_='description').text.strip()\n",
    "            rating = hostel.find('div', class_='score orange').text.strip()\n",
    "            total_reviews = hostel.find('div', class_='reviews').text.strip()\n",
    "            overall_reviews = hostel.find('div', class_='overall').text.strip()\n",
    "            privates_price = hostel.find('div', class_='prices').find('div', class_='privates from').text.strip()\n",
    "            dorms_price = hostel.find('div', class_='prices').find('div', class_='dorms from').text.strip()\n",
    "            facilities = hostel.find('ul', class_='facilities').text.strip()\n",
    "            description = hostel.find('div', class_='ratingtext').text.strip()\n",
    "\n",
    "            result = {\n",
    "                \"Hostel Name\": name,\n",
    "                \"Distance from City Centre\": distance,\n",
    "                \"Rating\": rating,\n",
    "                \"Total Reviews\": total_reviews,\n",
    "                \"Overall Reviews\": overall_reviews,\n",
    "                \"Privates from Price\": privates_price,\n",
    "                \"Dorms from Price\": dorms_price,\n",
    "                \"Facilities\": facilities,\n",
    "                \"Description\": description\n",
    "            }\n",
    "\n",
    "            results.append(result)\n",
    "\n",
    "        return results\n",
    "    else:\n",
    "        print(f\"Failed to fetch data from Hostelworld.com for {location}.\")\n",
    "        return []\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    location = input(\"Enter the location (e.g., London) to search for hostels: \")\n",
    "    hostels_data = scrape_hostels(location)\n",
    "\n",
    "    if hostels_data:\n",
    "        df = pd.DataFrame(hostels_data)\n",
    "        df.to_csv(f\"hostels_{location.lower()}.csv\", index=False)\n",
    "        print(f\"Scraping complete. Data saved to 'hostels_{location.lower()}.csv'\")\n",
    "    else:\n",
    "        print(f\"No data found for hostels in {location}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f7e47e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
